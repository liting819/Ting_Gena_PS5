---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Ting Tsai (liting)
    - Partner 2 (name and cnet ID): Genevieve Madigan (madigang)
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup

import warnings 
warnings.filterwarnings('ignore')
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
text = response.text
soup = BeautifulSoup(text, 'lxml')
```

Title:
```{python}
rows_title = []
titles = soup.find_all("h2", class_="usa-card__heading")
for title in titles:
    rows_title.append(title.get_text().strip())
```

Date:

```{python}
rows_date = []
divs = soup.find_all("div",class_ = "font-body-sm margin-top-1")
for div in divs:
  dates = div.find_all("span",class_ = "text-base-dark padding-right-105")
  for date in dates:
    rows_date.append(date.get_text().strip())
```

Catagories

```{python}
rows_cat = []
uls = soup.find_all("ul", class_="display-inline add-list-reset")
for ul in uls:
  cat =ul.find("li", class_ ="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
  rows_cat.append(cat.get_text())
```

Links:

```{python}
rows_url = []
titles = soup.find_all("h2", class_="usa-card__heading")
for title in titles:
    link = title.find("a")
    rows_url.append(link.get("href"))
```

```{python}
# combine into dataset
df = pd.DataFrame({"title":rows_title,
                    "date":rows_date,
                    "category":rows_cat,
                    "link":rows_url})
df.head()
```


### 2. Crawling (PARTNER 1)

```{python}
# store my links
titles = soup.find_all("h2", class_="usa-card__heading")
my_links=[]
for title in titles:
    link = title.find("a")
    my_links.append(link.get("href"))
```

```{python}
# go through the link and scrape the agency name 
rows_agency =[]
from urllib.parse import urljoin
for link in my_links:
    full_url = urljoin(base_url, link)
    response = requests.get(full_url)
    soup = BeautifulSoup(response.text, 'lxml')
    agency_section = soup.find("ul", class_="usa-list usa-list--unstyled margin-y-2")

    agency_name = "NA"
    
    if agency_section:
        for agency in agency_section.find_all("li"):
            if "Agency:" in agency.get_text():
                agency_name = agency.get_text(strip=True).replace("Agency:", "")
                break  
    rows_agency.append(agency_name)

```

```{python}
# add to the df and print head
df["agency"]= rows_agency
df.head()
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
Input Validation

If year < 2013:
Print a message: "Please enter a year greater than or equal to 2013."
Return from the function (no further processing).
Initialize Variables and Setup

Create the base URL of the HHS OIG “Enforcement Actions” page.
Initialize an empty list to store titles, dates, categories, links, and agency names.
Loop to Crawl Pages Until Todays Date

While there are more pages to crawl (starting from month and year up to todays date):
Send a request to the current page URL.

Parse the pages HTML content.

Loop through each enforcement action on the page (for each enforcement action entry):

Scrape title, date, category, and link.
Append each piece of data to the respective list.
Send a request to each actions link to scrape agency.
Append the agency name to the agency list.
Wait 1 second before requesting the next page (using time.sleep(1)).

Create a DataFrame from Lists

Use the pandas library to create a DataFrame with the scraped data.
Output Data

Generate a filename based on the year and month inputs.
Save the DataFrame to a .csv file with this filename.
End Function


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
```

```{python}

def scrape_all_pages(base_url, start_date):
    all_data = []
    page_number = 1

    while True:
        url = f"{base_url}?page={page_number}"
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        actions = soup.find_all('li', class_='usa-card')

        if not actions:
            break  

        for action in actions:
            title_tag = action.find('h2', class_='usa-card__heading').find('a')
            title = title_tag.get_text(strip=True)
            full_link = f"https://oig.hhs.gov{title_tag['href']}"

            
            date_text = action.find('span', class_='text-base-dark padding-right-105').get_text(strip=True)
            action_date = datetime.strptime(date_text, "%B %d, %Y")

            if action_date < start_date:
                return pd.DataFrame(all_data)  

            
            category = action.find('ul', class_='display-inline add-list-reset').get_text(strip=True)

            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': full_link})

        page_number += 1
        time.sleep(1)

    return pd.DataFrame(all_data)

def fetch_agency(link):
    try:
        response = requests.get(link)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        agency_section = soup.find("ul", class_="usa-list usa-list--unstyled margin-y-2")
        if agency_section:
            for item in agency_section.find_all('li'):
                if "Agency:" in item.get_text():
                    return item.get_text(strip=True).replace("Agency: ", "")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {link}: {e}")
    return 'N/A'

def get_agencies(df):
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows()}
        agencies = ['N/A'] * len(df)
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            agencies[index] = future.result()
    return agencies

def scrape_enforcement_actions(start_month, start_year):
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    start_date = datetime(start_year, start_month, 1)

    df = scrape_all_pages(base_url, start_date)
    df['Agency'] = get_agencies(df)
    
    
    filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

    return df

```

```{python}
df = scrape_enforcement_actions(1, 2023)
if df is not None:
    print(df.head())
```

The total number of enforcement actions collected is 1534 
Total enforcement actions collected is 1534
Earliest enforcement action:
Date: 2023-01-03 
Title: Podiatrist Pays $90,000 To Settle False Billing Allegations
Category: Criminal and Civil Actions
Agency: January 3, 2023
Link: https://oig.hhs.gov/fraud/enforcement/podiatrist-pays-90000-to-settle-false-billing-allegations/ 



* c. Test Partner's Code (PARTNER 1)

```{python}
df_2021 = scrape_enforcement_actions(1, 2021)
```

```{python}
# save the df into excel so currently we don't need to run again
df_2021.to_excel("df_2021.xlsx",index = False)
```

```{python}
len(df_2021)
```

```{python}
df_2021_sort = df_2021.sort_values(by = "Date",ascending = True)
df_2021_sort["Title"][0]
```


Total enforcement actions collected is 3022
Earliest enforcement action:
    Date: 2021-04-01
    Title: 'Red Rocks Radiation and Oncology, Alliance HealthCare Services, and Dr. Kevin Schewe Agreed to Pay $754,000 for Allegedly Violating the Civil Monetary Penalties Law by Submitting False Claims for Radiation and Oncology Services'
    Category: Fraud Self-Disclosures
    Agency: N/A
    Link: https://oig.hhs.gov/fraud/enforcement/red-rocks-radiation-and-oncology-alliance-healthcare-services-and-dr-kevin-schewe-agreed-to-pay-754000-for-allegedly-violating-the-civil-monetary-penalties-law-by-submitting-false-claims-for-radiation-and-oncology-services/


## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

# Load the Excel file into a DataFrame
df = pd.read_excel("df_2021.xlsx")

def plot_enforcement_actions_over_time(df, start_year=2021, start_month=1):
    # Convert 'Date' column to datetime
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    
    # Filter data to include only entries from January 2021 onward
    start_date = datetime(start_year, start_month, 1)
    df = df[df['Date'] >= start_date]
    
    # Group by month and year, and count enforcement actions per month
    df['YearMonth'] = df['Date'].dt.to_period('M')
    monthly_counts = df.groupby('YearMonth').size()

    # Plot the line chart
    plt.figure(figsize=(10, 6))
    monthly_counts.plot(kind='line', marker='o')
    plt.title("Number of Enforcement Actions Over Time (Aggregated by Month)")
    plt.xlabel("Month-Year")
    plt.ylabel("Number of Enforcement Actions")
    plt.xticks(rotation=45, ha='right')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Example usage:
plot_enforcement_actions_over_time(df)


```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
df_2021 = pd.read_excel("df_2021.xlsx")
df_2021_filtered = df_2021.loc[df_2021["Category"].isin(["Criminal and Civil Actions", "State Enforcement Agencies"])]

df_2021_filtered = df_2021_filtered.groupby(["Category","Date"]).agg( count = ("Title","count")).reset_index()
```

```{python}
import altair as alt 
from vega_datasets import data

# make a plot
chart = alt.Chart(df_2021_filtered).mark_line().encode(
    x='Date:T',               
    y='count:Q',             
    color='Category:N'        
).properties(
    title="Enforcement Action Over Time by Category",
    width=600,
    height=400
)
chart
```

* based on five topics

```{python}
# filter the data into only CCA category
df_cca = df_2021.loc[df_2021["Category"] == "Criminal and Civil Actions"].copy()

# defult sub title = other
df_cca["sub_cat"] = "Other" 

#assigned title for subtitle
df_cca.loc[df_cca["Title"].str.contains("Health Care", case=False, na=False), "sub_cat"] = "Health Care Fraud"
df_cca.loc[df_cca["Title"].str.contains("Financial", case=False, na=False), "sub_cat"] = "Financial Fraud"
df_cca.loc[df_cca["Title"].str.contains("Drug", case=False, na=False), "sub_cat"] = "Drug Enforcement"
df_cca.loc[df_cca["Title"].str.contains("Bribery|Corruption", case=False, na=False), "sub_cat"] = "Bribery/Corruption"
```

```{python}
# create the plot
df_cca_grouped  = df_cca.groupby(["sub_cat","Date"]).agg(count = ("Title","count")).reset_index()

chart = alt.Chart(df_cca_grouped).mark_line().encode(
    x='Date:T',               
    y='count:Q',             
    color='sub_cat:N'        
).properties(
    title="Criminal and Civic Action Over Time",
    width=600,
    height=400
)
chart
```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}
# create a column by state 
df_2021['State'] = df_2021['Agency'].str.extract(r'State of ([A-Za-z]+(?:\s[A-Za-z]+)?)', expand=False)
df_2021['State'].fillna("Not state level", inplace=True)

# grouped the dataset by state, filter out the not state level
df_2021_state = df_2021.loc[df_2021["State"]!= "Not state level"]
df_state_group = df_2021_state.groupby("State").agg(count = ("Title","count")).reset_index()
```

```{python}
# read the shape file and merge 
import geopandas as gpd
filepath = "/Users/tsaili-ting/Uchicago/Year2/Y2Fall/Python2/Ting_Gena_PS5/cb_2018_us_state_500k 2/cb_2018_us_state_500k.shx"
state_shp = gpd.read_file(filepath)

state_merge = state_shp.merge(df_state_group, left_on='NAME', right_on='State', how='left')
```

```{python}
# create choropleth of the number of enforcement action by state
state_merge.plot(column="count", cmap = "Blues",legend=True).set_axis_off()
```

### 2. Map by District (PARTNER 2)

```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import re
from difflib import get_close_matches
```


```{python}

# Load the U.S. Attorney District shapefile
shapefile_path = "/Users/tsaili-ting/Uchicago/Year2/Y2Fall/Python2/Ting_Gena_PS5/US Attorney Districts Shapefile simplified_20241110/geo_export_f5fd10c0-5f39-4a40-ab5f-d8894bfc7694.shx"
districts_shapefile = gpd.read_file(shapefile_path)

# Load enforcement actions data
df_2021 = pd.read_excel("df_2021.xlsx")

# Step 1: Extract and standardize district names from the "Agency" column
def extract_district(agency_text):
    match = re.search(r'(Eastern|Western|Northern|Southern|Middle)?\s*District\s+of\s+([A-Za-z\s]+)', str(agency_text))
    if match:
        return f"{match.group(1).strip()} District of {match.group(2).strip()}" if match.group(1) else f"District of {match.group(2).strip()}"
    return None

df_2021['District_Name'] = df_2021['Agency'].apply(extract_district)

# Unique district names in df_2021
district_names_in_data = df_2021['District_Name'].dropna().unique()

# Unique district identifiers in shapefile
district_names_in_shapefile = districts_shapefile['district_n'].unique()

# Step 2: Automated Mapping using get_close_matches
district_mapping = {}
for district in district_names_in_data:
    match = get_close_matches(district, district_names_in_shapefile, n=1, cutoff=0.6)
    if match:
        district_mapping[district] = match[0]

# Map the extracted district names to identifiers in shapefile
df_2021['District_ID'] = df_2021['District_Name'].map(district_mapping)

# Step 3: Aggregate the number of enforcement actions by district ID
district_counts = df_2021.groupby('District_ID').size().reset_index(name='Enforcement_Count')

# Step 4: Merge with the shapefile based on district identifiers
merged = districts_shapefile.merge(district_counts, left_on='district_n', right_on='District_ID', how='left')
merged['Enforcement_Count'] = merged['Enforcement_Count'].fillna(0)  # Fill NaN values with 0 for districts with no data

# Step 5: Plot the choropleth map
fig, ax = plt.subplots(1, 1, figsize=(15, 10))
merged.plot(column='Enforcement_Count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
plt.title('Number of Enforcement Actions by U.S. Attorney District')
plt.axis('off')
plt.show()


```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```