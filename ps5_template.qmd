---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Ting Tsai (liting)
    - Partner 2 (name and cnet ID): Genevieve Madigan (madigang)
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
import requests
from bs4 import BeautifulSoup
```

```{python}
with open(r'/Users/tsaili-ting/Uchicago/Year2/Y2Fall/Python2/Ting_Gena_PS5/Enforcement Actions.html', 'r') as page:
    text = page.read()
soup = BeautifulSoup(text, 'lxml')
```

Title:
```{python}
titles = soup.find_all("h2", class_="usa-card__heading")
for title in titles[0:5]:
    print(title.get_text())
```

Date:

```{python}
divs = soup.find_all("div",class_ = "font-body-sm margin-top-1")
for div in divs:
  dates = div.find_all("span",class_ = "text-base-dark padding-right-105")
  for date in dates:
    print(date.get_text())
```

Catagories

```{python}
uls = soup.find_all("ul", class_="display-inline add-list-reset")
for ul in uls:
  cat =ul.find("li", class_ ="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1")
  print(cat.get_text())
```

Links:

```{python}
titles = soup.find_all("h2", class_="usa-card__heading")
for title in titles:
    link = title.find("a")
    print(link.get("href"))
```

### 2. Crawling (PARTNER 1)

```{python}
titles = soup.find_all("h2", class_="usa-card__heading")
my_links=[]
for title in titles:
    link = title.find("a")
    my_links.append(link.get("href"))
```

```{python}
for link in my_links:
  response = requests.get(link)
  soup = BeautifulSoup(response.text, 'lxml')
  for i in range(50):
    uls = soup.find_all("ul", class_= "usa-list usa-list--unstyled margin-y-2")
    for ul in uls:
      lis = ul.find_all("span", class_="padding-right-2 text-base")

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
Input Validation

If year < 2013:
Print a message: "Please enter a year greater than or equal to 2013."
Return from the function (no further processing).
Initialize Variables and Setup

Create the base URL of the HHS OIG “Enforcement Actions” page.
Initialize an empty list to store titles, dates, categories, links, and agency names.
Loop to Crawl Pages Until Todays Date

While there are more pages to crawl (starting from month and year up to todays date):
Send a request to the current page URL.

Parse the pages HTML content.

Loop through each enforcement action on the page (for each enforcement action entry):

Scrape title, date, category, and link.
Append each piece of data to the respective list.
Send a request to each actions link to scrape agency.
Append the agency name to the agency list.
Wait 1 second before requesting the next page (using time.sleep(1)).

Create a DataFrame from Lists

Use the pandas library to create a DataFrame with the scraped data.
Output Data

Generate a filename based on the year and month inputs.
Save the DataFrame to a .csv file with this filename.
End Function


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

```

```{python}

def scrape_all_pages(base_url, start_date):
    all_data = []
    page_number = 1

    while True:
        url = f"{base_url}?page={page_number}"
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        actions = soup.find_all('li', class_='usa-card')

        if not actions:
            break  

        for action in actions:
            title_tag = action.find('h2', class_='usa-card__heading').find('a')
            title = title_tag.get_text(strip=True)
            full_link = f"https://oig.hhs.gov{title_tag['href']}"

            
            date_text = action.find('span', class_='text-base-dark padding-right-105').get_text(strip=True)
            action_date = datetime.strptime(date_text, "%B %d, %Y")

            if action_date < start_date:
                return pd.DataFrame(all_data)  

            
            category = action.find('ul', class_='display-inline add-list-reset').get_text(strip=True)

            all_data.append({'Title': title, 'Date': date_text, 'Category': category, 'Link': full_link})

        page_number += 1
        time.sleep(1)

    return pd.DataFrame(all_data)

def fetch_agency(link):
    try:
        response = requests.get(link)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        agency_section = soup.find("ul", class_="usa-list usa-list--unstyled margin-y-2")
        if agency_section:
            for item in agency_section.find_all('li'):
                if "Agency:" in item.get_text():
                    return item.get_text(strip=True).replace("Agency: ", "")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {link}: {e}")
    return 'N/A'

def get_agencies(df):
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_index = {executor.submit(fetch_agency, row['Link']): index for index, row in df.iterrows()}
        agencies = ['N/A'] * len(df)
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            agencies[index] = future.result()
    return agencies

def scrape_enforcement_actions(start_month, start_year):
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    start_date = datetime(start_year, start_month, 1)

    df = scrape_all_pages(base_url, start_date)
    df['Agency'] = get_agencies(df)
    
    
    filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    df.to_csv(filename, index=False)
    print(f"Data saved to {filename}")

    return df


df = scrape_enforcement_actions(1, 2023)
if df is not None:
    print(df.head())


```

The total number of enforcement actions collected is 1534 
Total enforcement actions collected is 1534
Earliest enforcement action:
Date: 2023-01-03 
Title: Podiatrist Pays $90,000 To Settle False Billing Allegations
Category: Criminal and Civil Actions
Agency: January 3, 2023
Link: https://oig.hhs.gov/fraud/enforcement/podiatrist-pays-90000-to-settle-false-billing-allegations/ 





* c. Test Partner's Code (PARTNER 1)

```{python}


```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```